# FCMR æ•°æ®é›†æ›¿ä»£ä»»åŠ¡å®šä¹‰ä¸åº”ç”¨åœºæ™¯

æœ¬æ–‡æ¡£åˆ—å‡ºåŸºäº FCMR æ•°æ®é›†å¯ä»¥å®šä¹‰çš„å„ç§ä»»åŠ¡ã€æŒ‡æ ‡å’Œåº”ç”¨åœºæ™¯ï¼Œ**ä¸ç›´æ¥å¯¹æ¯”åŸè®ºæ–‡çš„ LLM baseline**ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ä»»åŠ¡å®šä¹‰](#ä»»åŠ¡å®šä¹‰)
2. [æ‰©å±•æŒ‡æ ‡](#æ‰©å±•æŒ‡æ ‡)
3. [åº”ç”¨åœºæ™¯](#åº”ç”¨åœºæ™¯)
4. [å®ç°å»ºè®®](#å®ç°å»ºè®®)

---

## ğŸ¯ ä»»åŠ¡å®šä¹‰

### 1. éš¾åº¦é¢„æµ‹ä»»åŠ¡ (Difficulty Prediction)

**ä»»åŠ¡æè¿°**: é¢„æµ‹é¢˜ç›®çš„éš¾åº¦çº§åˆ«ï¼ˆEasy/Medium/Hardï¼‰

**æ ‡ç­¾æ„å»º**:
```python
# ä» FCMR æ•°æ®ä¸­çš„ difficulty å­—æ®µ
difficulty_map = {'easy': 0, 'medium': 1, 'hard': 2}
labels = df['difficulty'].map(difficulty_map).values
```

**è¾“å‡ºç»´åº¦**: `output_dim=3` (ä¸‰åˆ†ç±»)

**é€‚ç”¨æŒ‡æ ‡**:
- Accuracy, F1_Macro, F1_Micro
- AUC-ROC (one-vs-rest), AUC-PR
- Top-K Accuracy (K=2, 3)
- Cohen's Kappa, MCC

**åº”ç”¨ä»·å€¼**: 
- è¯„ä¼°æ¨¡å‹å¯¹é¢˜ç›®å¤æ‚åº¦çš„ç†è§£èƒ½åŠ›
- å¯ç”¨äºè‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ
- é‡‘èé£é™©è¯„ä¼°ï¼ˆç®€å•â†’é«˜é£é™©ï¼Œå¤æ‚â†’éœ€æ·±å…¥åˆ†æï¼‰

---

### 2. ç­”æ¡ˆæ•°é‡é¢„æµ‹ (Answer Count Prediction)

**ä»»åŠ¡æè¿°**: é¢„æµ‹æ­£ç¡®ç­”æ¡ˆçš„æ•°é‡ï¼ˆ0-3ä¸ªï¼‰

**æ ‡ç­¾æ„å»º**:
```python
# ä» answer å­—æ®µè§£æç­”æ¡ˆæ•°é‡
def count_answers(answer_str):
    if answer_str == 'None' or pd.isna(answer_str):
        return 0
    return len(str(answer_str).split(','))
labels = df['answer'].apply(count_answers).values
```

**è¾“å‡ºç»´åº¦**: `output_dim=4` (0, 1, 2, 3ä¸ªç­”æ¡ˆ)

**é€‚ç”¨æŒ‡æ ‡**:
- Accuracy, F1_Macro
- AUC-ROC, AUC-PR
- Mean Absolute Error (ä½œä¸ºå›å½’ä»»åŠ¡)
- Spearman Correlation (å¦‚æœä½œä¸ºæ’åºä»»åŠ¡)

**åº”ç”¨ä»·å€¼**:
- è¯„ä¼°æ¨¡å‹å¯¹é—®é¢˜å¤æ‚åº¦çš„é‡åŒ–ç†è§£
- ä¿¡æ¯æ£€ç´¢ï¼šé¢„æµ‹éœ€è¦æ£€ç´¢å¤šå°‘ä¸ªç›¸å…³æ–‡æ¡£
- é‡‘èåˆ†æï¼šé¢„æµ‹éœ€è¦å…³æ³¨å¤šå°‘ä¸ªå…³é”®æŒ‡æ ‡

---

### 3. æ¨¡æ€é‡è¦æ€§æ’åº (Modality Importance Ranking)

**ä»»åŠ¡æè¿°**: é¢„æµ‹å“ªä¸ªæ¨¡æ€ï¼ˆæ–‡æœ¬/è¡¨æ ¼/å›¾è¡¨ï¼‰å¯¹ç­”æ¡ˆæœ€é‡è¦

**æ ‡ç­¾æ„å»º**:
```python
# åŸºäºç­”æ¡ˆä¸å„æ¨¡æ€çš„ç›¸å…³æ€§æ„å»ºæ ‡ç­¾
# æ–¹æ³•1: ä½¿ç”¨æ³¨æ„åŠ›æƒé‡ï¼ˆéœ€è¦è®­ç»ƒååˆ†æï¼‰
# æ–¹æ³•2: åŸºäºç­”æ¡ˆç±»å‹ï¼ˆæ–‡æœ¬ç­”æ¡ˆâ†’æ–‡æœ¬æ¨¡æ€é‡è¦ï¼Œæ•°å€¼ç­”æ¡ˆâ†’è¡¨æ ¼é‡è¦ï¼‰
modality_importance = [0.4, 0.3, 0.3]  # [text, table, chart]
```

**è¾“å‡ºç»´åº¦**: `output_dim=3` (æ¯ä¸ªæ¨¡æ€çš„é‡è¦æ€§åˆ†æ•°ï¼Œå½’ä¸€åŒ–åˆ°[0,1])

**é€‚ç”¨æŒ‡æ ‡**:
- NDCG@K (K=3)
- Mean Reciprocal Rank (MRR)
- Mean Average Precision (MAP)
- Spearman/Kendall Correlation (ä¸çœŸå®é‡è¦æ€§æ’åº)

**åº”ç”¨ä»·å€¼**:
- å¯è§£é‡Šæ€§åˆ†æï¼šå“ªäº›æ¨¡æ€å¯¹å†³ç­–æœ€é‡è¦
- èµ„æºåˆ†é…ï¼šä¼˜å…ˆå¤„ç†é‡è¦æ¨¡æ€
- å¤šæ¨¡æ€èåˆç­–ç•¥ä¼˜åŒ–

---

### 4. ç­”æ¡ˆç½®ä¿¡åº¦é¢„æµ‹ (Answer Confidence Prediction)

**ä»»åŠ¡æè¿°**: é¢„æµ‹æ¨¡å‹å¯¹ç­”æ¡ˆçš„ç½®ä¿¡åº¦ï¼ˆå›å½’ä»»åŠ¡ï¼‰

**æ ‡ç­¾æ„å»º**:
```python
# åŸºäºéš¾åº¦å’Œç­”æ¡ˆæ•°é‡æ„å»ºç½®ä¿¡åº¦æ ‡ç­¾
# ç®€å• + å•ç­”æ¡ˆ â†’ é«˜ç½®ä¿¡åº¦
# å›°éš¾ + å¤šç­”æ¡ˆ â†’ ä½ç½®ä¿¡åº¦
def calculate_confidence(difficulty, answer_count):
    base_conf = {'easy': 0.9, 'medium': 0.6, 'hard': 0.3}[difficulty]
    penalty = answer_count * 0.1  # å¤šç­”æ¡ˆé™ä½ç½®ä¿¡åº¦
    return max(0.1, base_conf - penalty)
labels = df.apply(lambda r: calculate_confidence(r['difficulty'], count_answers(r['answer'])), axis=1)
```

**è¾“å‡ºç»´åº¦**: `output_dim=1` (è¿ç»­å€¼ [0, 1])

**é€‚ç”¨æŒ‡æ ‡**:
- MSE, MAE, RMSE, RÂ²
- Pearson/Spearman/Kendall Correlation
- AUC-ROC (è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼šé«˜/ä½ç½®ä¿¡åº¦)
- Information Ratio (é‡‘èæŒ‡æ ‡)

**åº”ç”¨ä»·å€¼**:
- ä¸ç¡®å®šæ€§é‡åŒ–ï¼šæ¨¡å‹ä½•æ—¶ä¸ç¡®å®š
- ä¸»åŠ¨å­¦ä¹ ï¼šä¼˜å…ˆæ ‡æ³¨ä½ç½®ä¿¡åº¦æ ·æœ¬
- é£é™©æ§åˆ¶ï¼šä½ç½®ä¿¡åº¦æ—¶è§¦å‘äººå·¥å®¡æ ¸

---

### 5. å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ (Anomaly Detection)

**ä»»åŠ¡æè¿°**: æ£€æµ‹å¤šæ¨¡æ€æ•°æ®ä¸­çš„å¼‚å¸¸æ ·æœ¬ï¼ˆä¸æ­£å¸¸æ¨¡å¼ä¸ç¬¦ï¼‰

**æ ‡ç­¾æ„å»º**:
```python
# æ–¹æ³•1: åŸºäºç­”æ¡ˆåˆ†å¸ƒï¼ˆç½•è§ç­”æ¡ˆç»„åˆ â†’ å¼‚å¸¸ï¼‰
answer_counts = df['answer'].value_counts()
rare_threshold = answer_counts.quantile(0.1)
labels = (df['answer'].map(answer_counts) < rare_threshold).astype(int)

# æ–¹æ³•2: åŸºäºæ¨¡æ€ç‰¹å¾å¼‚å¸¸ï¼ˆä½¿ç”¨ Isolation Forestï¼‰
from sklearn.ensemble import IsolationForest
# æå–æ¨¡æ€ç‰¹å¾åæ£€æµ‹å¼‚å¸¸
```

**è¾“å‡ºç»´åº¦**: `output_dim=1` (äºŒåˆ†ç±»ï¼šæ­£å¸¸/å¼‚å¸¸) æˆ– `output_dim=1` (å¼‚å¸¸åˆ†æ•°)

**é€‚ç”¨æŒ‡æ ‡**:
- AUC-ROC, AUC-PR
- Precision@K (Top-K å¼‚å¸¸æ£€æµ‹)
- F1_Score (å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡)
- Silhouette Score (èšç±»è´¨é‡ï¼Œå¦‚æœä½¿ç”¨æ— ç›‘ç£)

**åº”ç”¨ä»·å€¼**:
- é‡‘èæ¬ºè¯ˆæ£€æµ‹ï¼šå¼‚å¸¸äº¤æ˜“æ¨¡å¼
- æ•°æ®è´¨é‡ç›‘æ§ï¼šè¯†åˆ«é”™è¯¯æ ‡æ³¨æˆ–å™ªå£°æ•°æ®
- é£é™©é¢„è­¦ï¼šå¼‚å¸¸å¸‚åœºä¿¡å·

---

### 6. æ¨¡æ€å¯¹é½è´¨é‡è¯„ä¼° (Modality Alignment Quality)

**ä»»åŠ¡æè¿°**: è¯„ä¼°ä¸‰ä¸ªæ¨¡æ€ä¹‹é—´çš„å¯¹é½/ä¸€è‡´æ€§ç¨‹åº¦

**æ ‡ç­¾æ„å»º**:
```python
# åŸºäºæ¨¡æ€ç‰¹å¾ç›¸ä¼¼åº¦
def calculate_alignment(text_feat, table_feat, chart_feat):
    # è®¡ç®—æ¨¡æ€é—´ä½™å¼¦ç›¸ä¼¼åº¦
    sim_text_table = cosine_similarity(text_feat, table_feat)
    sim_text_chart = cosine_similarity(text_feat, chart_feat)
    sim_table_chart = cosine_similarity(table_feat, chart_feat)
    return (sim_text_table + sim_text_chart + sim_table_chart) / 3
```

**è¾“å‡ºç»´åº¦**: `output_dim=1` (å¯¹é½åˆ†æ•° [0, 1])

**é€‚ç”¨æŒ‡æ ‡**:
- MSE, MAE, RÂ²
- Pearson/Spearman Correlation
- AUC-ROC (è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼šå¯¹é½/ä¸å¯¹é½)

**åº”ç”¨ä»·å€¼**:
- æ•°æ®è´¨é‡è¯„ä¼°ï¼šå¤šæ¨¡æ€æ•°æ®æ˜¯å¦ä¸€è‡´
- èåˆç­–ç•¥é€‰æ‹©ï¼šå¯¹é½å¥½çš„æ•°æ®ç”¨ç®€å•èåˆï¼Œå¯¹é½å·®çš„ç”¨å¤æ‚èåˆ
- é¢„è®­ç»ƒæ•°æ®ç­›é€‰ï¼šé€‰æ‹©å¯¹é½è´¨é‡é«˜çš„æ•°æ®

---

### 7. ç­”æ¡ˆç±»å‹åˆ†ç±» (Answer Type Classification)

**ä»»åŠ¡æè¿°**: é¢„æµ‹ç­”æ¡ˆç±»å‹ï¼ˆå•é€‰é¡¹/å¤šé€‰é¡¹/æ— ç­”æ¡ˆï¼‰

**æ ‡ç­¾æ„å»º**:
```python
def get_answer_type(answer_str):
    if pd.isna(answer_str) or answer_str == 'None':
        return 0  # æ— ç­”æ¡ˆ
    answer_list = str(answer_str).split(',')
    if len(answer_list) == 1:
        return 1  # å•é€‰é¡¹
    else:
        return 2  # å¤šé€‰é¡¹
labels = df['answer'].apply(get_answer_type).values
```

**è¾“å‡ºç»´åº¦**: `output_dim=3` (ä¸‰åˆ†ç±»)

**é€‚ç”¨æŒ‡æ ‡**:
- Accuracy, F1_Macro, F1_Micro
- AUC-ROC, AUC-PR
- Top-K Accuracy
- Cohen's Kappa, MCC

**åº”ç”¨ä»·å€¼**:
- é—®é¢˜ç±»å‹è¯†åˆ«ï¼šç®€å•é—®é¢˜ vs å¤æ‚é—®é¢˜
- æ£€ç´¢ç­–ç•¥ï¼šå•ç­”æ¡ˆç”¨ç²¾ç¡®åŒ¹é…ï¼Œå¤šç­”æ¡ˆç”¨æ¨¡ç³ŠåŒ¹é…
- ç”¨æˆ·ç•Œé¢ï¼šæ ¹æ®ç­”æ¡ˆç±»å‹è°ƒæ•´å±•ç¤ºæ–¹å¼

---

### 8. æ¨¡æ€ç¼ºå¤±é²æ£’æ€§è¯„ä¼° (Missing Modality Robustness)

**ä»»åŠ¡æè¿°**: è¯„ä¼°æ¨¡å‹åœ¨æŸä¸ªæ¨¡æ€ç¼ºå¤±æ—¶çš„æ€§èƒ½

**æ ‡ç­¾æ„å»º**:
```python
# è®­ç»ƒæ—¶éšæœº mask æŸä¸ªæ¨¡æ€ï¼ˆç½®é›¶ï¼‰
# æµ‹è¯•æ—¶è¯„ä¼°ä¸åŒç¼ºå¤±æ¨¡å¼ä¸‹çš„æ€§èƒ½
# ä¸éœ€è¦é¢å¤–æ ‡ç­¾ï¼Œä½¿ç”¨åŸå§‹ç­”æ¡ˆæ ‡ç­¾
```

**è¾“å‡ºç»´åº¦**: ä¸åŸå§‹ä»»åŠ¡ç›¸åŒï¼ˆåˆ†ç±»/å›å½’ï¼‰

**é€‚ç”¨æŒ‡æ ‡**:
- å¯¹æ¯”å®Œæ•´æ¨¡æ€ vs ç¼ºå¤±æ¨¡æ€çš„æ€§èƒ½ä¸‹é™
- å„æ¨¡å‹çš„é²æ£’æ€§æ’å
- æ¨¡æ€é‡è¦æ€§åˆ†æï¼ˆç¼ºå¤±å“ªä¸ªæ¨¡æ€å½±å“æœ€å¤§ï¼‰

**åº”ç”¨ä»·å€¼**:
- å®é™…éƒ¨ç½²ï¼šå¤„ç†æ¨¡æ€ç¼ºå¤±åœºæ™¯
- æ¨¡å‹é€‰æ‹©ï¼šé€‰æ‹©å¯¹ç¼ºå¤±æœ€é²æ£’çš„æ¨¡å‹
- èµ„æºä¼˜åŒ–ï¼šä¼˜å…ˆä¿è¯é‡è¦æ¨¡æ€çš„è´¨é‡

---

## ğŸ“Š æ‰©å±•æŒ‡æ ‡

### åˆ†ç±»ä»»åŠ¡æ‰©å±•æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **AUC-ROC** | ROCæ›²çº¿ä¸‹é¢ç§¯ | äºŒåˆ†ç±»/å¤šåˆ†ç±»ï¼ˆone-vs-restï¼‰ |
| **AUC-PR** | Precision-Recallæ›²çº¿ä¸‹é¢ç§¯ | ä¸å¹³è¡¡æ•°æ®é›† |
| **Top-K Accuracy** | Top-Ké¢„æµ‹å‡†ç¡®ç‡ | å…è®¸Kä¸ªå€™é€‰ç­”æ¡ˆ |
| **Cohen's Kappa** | è€ƒè™‘éšæœºä¸€è‡´æ€§çš„å‡†ç¡®ç‡ | ç±»åˆ«ä¸å¹³è¡¡ |
| **MCC** | Matthewsç›¸å…³ç³»æ•° | äºŒåˆ†ç±»/å¤šåˆ†ç±»å¹³è¡¡è¯„ä¼° |
| **Hamming Loss** | å¤šæ ‡ç­¾åˆ†ç±»é”™è¯¯ç‡ | å¤šæ ‡ç­¾ä»»åŠ¡ |

### å›å½’ä»»åŠ¡æ‰©å±•æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **Pearson R** | çº¿æ€§ç›¸å…³ç³»æ•° | çº¿æ€§å…³ç³»è¯„ä¼° |
| **Spearman R** | ç§©ç›¸å…³ç³»æ•° | å•è°ƒå…³ç³»è¯„ä¼° |
| **Kendall Tau** | æ’åºä¸€è‡´æ€§ | æ’åºä»»åŠ¡ |
| **MAPE** | å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® | ç›¸å¯¹è¯¯å·®è¯„ä¼° |
| **Information Ratio** | ä¿¡æ¯æ¯”ç‡ | é‡‘èé¢„æµ‹ |
| **Sharpe Ratio** | å¤æ™®æ¯”ç‡ | é‡‘èæ”¶ç›Šè¯„ä¼° |
| **Hit Rate** | æ–¹å‘é¢„æµ‹å‡†ç¡®ç‡ | è¶‹åŠ¿é¢„æµ‹ |

### æ’åºä»»åŠ¡æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **NDCG@K** | å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š | æ¨èç³»ç»Ÿã€æ£€ç´¢ |
| **MRR** | å¹³å‡å€’æ•°æ’å | æ£€ç´¢ä»»åŠ¡ |
| **MAP** | å¹³å‡ç²¾åº¦å‡å€¼ | å¤šæ ‡ç­¾æ’åº |

### èšç±»/å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **Silhouette Score** | è½®å»“ç³»æ•° | èšç±»è´¨é‡ |
| **Calinski-Harabasz** | CHæŒ‡æ•° | èšç±»åˆ†ç¦»åº¦ |
| **Davies-Bouldin** | DBæŒ‡æ•° | èšç±»ç´§å¯†åº¦ |

---

## ğŸš€ åº”ç”¨åœºæ™¯

### 1. é‡‘èé£é™©è¯„ä¼°

**ä»»åŠ¡**: éš¾åº¦é¢„æµ‹ + ç½®ä¿¡åº¦é¢„æµ‹

**åº”ç”¨**:
- **é£é™©åˆ†çº§**: Easy â†’ ä½é£é™©ï¼ŒHard â†’ é«˜é£é™©
- **ç½®ä¿¡åº¦é˜ˆå€¼**: ä½ç½®ä¿¡åº¦æ—¶è§¦å‘äººå·¥å®¡æ ¸
- **è‡ªåŠ¨åŒ–å†³ç­–**: é«˜ç½®ä¿¡åº¦ + Easy â†’ è‡ªåŠ¨é€šè¿‡

**æŒ‡æ ‡**: Accuracy, AUC-ROC, Information Ratio

---

### 2. æ™ºèƒ½é—®ç­”ç³»ç»Ÿ

**ä»»åŠ¡**: ç­”æ¡ˆæ•°é‡é¢„æµ‹ + æ¨¡æ€é‡è¦æ€§æ’åº

**åº”ç”¨**:
- **æ£€ç´¢ç­–ç•¥**: é¢„æµ‹éœ€è¦æ£€ç´¢å¤šå°‘ä¸ªæ–‡æ¡£
- **æ¨¡æ€ä¼˜å…ˆçº§**: ä¼˜å…ˆå¤„ç†é‡è¦æ¨¡æ€
- **ç­”æ¡ˆç”Ÿæˆ**: æ ¹æ®ç­”æ¡ˆæ•°é‡è°ƒæ•´ç”Ÿæˆç­–ç•¥

**æŒ‡æ ‡**: Accuracy, NDCG@K, MRR

---

### 3. æ•°æ®è´¨é‡ç›‘æ§

**ä»»åŠ¡**: å¼‚å¸¸æ£€æµ‹ + æ¨¡æ€å¯¹é½è´¨é‡è¯„ä¼°

**åº”ç”¨**:
- **å¼‚å¸¸æ ·æœ¬æ£€æµ‹**: è¯†åˆ«æ ‡æ³¨é”™è¯¯æˆ–å™ªå£°æ•°æ®
- **æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥**: å¤šæ¨¡æ€æ•°æ®æ˜¯å¦å¯¹é½
- **é¢„è®­ç»ƒæ•°æ®ç­›é€‰**: é€‰æ‹©é«˜è´¨é‡æ•°æ®

**æŒ‡æ ‡**: AUC-ROC, AUC-PR, Silhouette Score

---

### 4. è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ

**ä»»åŠ¡**: éš¾åº¦é¢„æµ‹ + ç­”æ¡ˆç½®ä¿¡åº¦é¢„æµ‹

**åº”ç”¨**:
- **ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„**: æ ¹æ®éš¾åº¦è°ƒæ•´å­¦ä¹ å†…å®¹
- **ä¸»åŠ¨å­¦ä¹ **: ä¼˜å…ˆæ ‡æ³¨ä½ç½®ä¿¡åº¦æ ·æœ¬
- **çŸ¥è¯†è¿½è¸ª**: è·Ÿè¸ªå­¦ä¹ è€…å¯¹ä¸åŒéš¾åº¦é¢˜ç›®çš„æŒæ¡ç¨‹åº¦

**æŒ‡æ ‡**: Accuracy, F1_Macro, AUC-ROC

---

### 5. å¤šæ¨¡æ€èåˆç­–ç•¥ä¼˜åŒ–

**ä»»åŠ¡**: æ¨¡æ€é‡è¦æ€§æ’åº + æ¨¡æ€ç¼ºå¤±é²æ£’æ€§è¯„ä¼°

**åº”ç”¨**:
- **èåˆæƒé‡è°ƒæ•´**: æ ¹æ®é‡è¦æ€§åŠ¨æ€è°ƒæ•´èåˆæƒé‡
- **æ¨¡å‹é€‰æ‹©**: é€‰æ‹©å¯¹ç¼ºå¤±æœ€é²æ£’çš„æ¨¡å‹
- **èµ„æºåˆ†é…**: ä¼˜å…ˆä¿è¯é‡è¦æ¨¡æ€çš„è´¨é‡

**æŒ‡æ ‡**: NDCG@K, Spearman R, æ€§èƒ½ä¸‹é™ç‡

---

### 6. é‡‘èè¶‹åŠ¿é¢„æµ‹

**ä»»åŠ¡**: ç­”æ¡ˆç½®ä¿¡åº¦é¢„æµ‹ï¼ˆä½œä¸ºå›å½’ï¼‰+ æ–¹å‘é¢„æµ‹

**åº”ç”¨**:
- **è‚¡ä»·è¶‹åŠ¿**: é¢„æµ‹æ¶¨è·Œæ–¹å‘ï¼ˆHit Rateï¼‰
- **é£é™©è¯„ä¼°**: ç½®ä¿¡åº¦ä½æ—¶é™ä½ä»“ä½
- **äº¤æ˜“ç­–ç•¥**: é«˜ç½®ä¿¡åº¦ + æ˜ç¡®æ–¹å‘ â†’ æ‰§è¡Œäº¤æ˜“

**æŒ‡æ ‡**: Hit Rate, Information Ratio, Sharpe Ratio

---

### 7. å¯è§£é‡Šæ€§åˆ†æ

**ä»»åŠ¡**: æ¨¡æ€é‡è¦æ€§æ’åº + ç­”æ¡ˆç±»å‹åˆ†ç±»

**åº”ç”¨**:
- **å†³ç­–è§£é‡Š**: å±•ç¤ºå“ªäº›æ¨¡æ€å¯¹å†³ç­–æœ€é‡è¦
- **ç”¨æˆ·ç†è§£**: å¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹
- **æ¨¡å‹è°ƒè¯•**: è¯†åˆ«æ¨¡å‹ä¾èµ–çš„æ¨¡æ€ç‰¹å¾

**æŒ‡æ ‡**: NDCG@K, æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–

---

## ğŸ’» å®ç°å»ºè®®

### 1. å¿«é€Ÿåˆ‡æ¢ä»»åŠ¡

åœ¨ `utils/load_fcmr.py` ä¸­æ·»åŠ ä»»åŠ¡é€‰æ‹©å‚æ•°ï¼š

```python
def load_as_multimodal(self, extract_features=True, task='original'):
    """
    task options:
    - 'original': åŸå§‹å¤šæ ‡ç­¾åˆ†ç±»
    - 'difficulty': éš¾åº¦é¢„æµ‹
    - 'answer_count': ç­”æ¡ˆæ•°é‡é¢„æµ‹
    - 'confidence': ç½®ä¿¡åº¦é¢„æµ‹
    - 'anomaly': å¼‚å¸¸æ£€æµ‹
    - 'alignment': æ¨¡æ€å¯¹é½è´¨é‡
    """
    if task == 'difficulty':
        labels = self._encode_difficulty()
    elif task == 'answer_count':
        labels = self._encode_answer_count()
    # ... å…¶ä»–ä»»åŠ¡
```

### 2. æŒ‡æ ‡è®¡ç®—é›†æˆ

åœ¨ `train.py` ä¸­é›†æˆæ‰©å±•æŒ‡æ ‡ï¼š

```python
from utils.extended_metrics import calculate_all_extended_metrics

# è®¡ç®—æ‰©å±•æŒ‡æ ‡
extended_metrics = calculate_all_extended_metrics(
    y_true=val_labels,
    y_pred=val_predictions,
    task_type=config['task_type'],
    is_multilabel=is_multilabel,
    modality_features=[mod1, mod2, mod3],
    k=5
)
```

### 3. é…ç½®æ–‡ä»¶æ‰©å±•

åœ¨ `configs/config_fcmr_*.yaml` ä¸­æ·»åŠ ä»»åŠ¡é…ç½®ï¼š

```yaml
task:
  name: difficulty_prediction  # æˆ– answer_count, confidence, etc.
  output_dim: 3
  task_type: classification
  
metrics:
  standard: [Accuracy, F1_Macro, F1_Micro]
  extended: [AUC-ROC, AUC-PR, Top-5_Accuracy, Cohen_Kappa]
  ranking: [NDCG@3, MRR, MAP]  # å¦‚æœé€‚ç”¨
  correlation: [Pearson_R, Spearman_R]  # å¦‚æœé€‚ç”¨
```

---

## ğŸ“ è®ºæ–‡å†™ä½œå»ºè®®

### å®éªŒéƒ¨åˆ†å¯ä»¥è¿™æ ·å†™ï¼š

> **Alternative Task Evaluations on FCMR Dataset**
> 
> Beyond the original multi-label classification task, we evaluate our models on several alternative tasks that better align with practical applications:
> 
> 1. **Difficulty Prediction**: Predicting the difficulty level (Easy/Medium/Hard) of each question, which is useful for adaptive learning systems and risk assessment.
> 
> 2. **Answer Count Prediction**: Predicting how many correct answers exist (0-3), which helps in information retrieval and question understanding.
> 
> 3. **Modality Importance Ranking**: Ranking the importance of each modality (text/table/chart) for the final answer, providing interpretability insights.
> 
> 4. **Answer Confidence Prediction**: Predicting model confidence as a regression task, enabling uncertainty quantification and active learning.
> 
> We report **AUC-ROC**, **NDCG@K**, **Spearman correlation**, and **Top-K accuracy** metrics, which are more suitable for these tasks than the original FCMR metrics.

---

**æœ€åæ›´æ–°**: 2026-01-29
